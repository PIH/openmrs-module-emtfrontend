Overview

Simplistic system and OpenMRS uptime and resouce monitoring for MOH Rwanda with the EMT (EMR Monitoring Tool).

It contains three different components, which are all stored in the project openmrs-module-emtfrontend:
1. Shell data collection
2. Java reporting
3. OpenMRS module


Shell data collection

The shell data collection is responsible for logging relevant system and OpenMRS uptime and resource monitoring events into the EMT log file. This is done by multiple dedicated shell scripts. These scripts are invoked by different cronjobs. 

The script startup-hook.sh is executed during boot/reboot of the server. First it checks for a proper/successful preceding system shutdown. If it finds a previous shutdown, it adds a matching SHUTDOWN event followed by a STARTUP event into the log file. If no previous shutdown can be detected, only a STARTUP event with a DIRTY indication is logged.

The main system availability is checked through the script heartbeat.sh. Apart from the (obvious) ping if the system is alive (which is nevertheless crucial as otherwise there is barely any other events which contains this information), some statistics about system load, and memory and disk usuage are collected and logged.

The OpenMRS installation and its database is checkd by the script openmrs-heartbeat.sh. Several things are checked; for most if these checks values from the local openmrs-runtime.properties are used:
1. Is a web login to the local OpenMRS server possible? If not, it waits for another minutes before it retries. If both login attempt are not successful, then an OpenMRS NOT RESPONDING event is logged.
2. Afterwards a couple of indicators (numbers of encounters, obs, users, are calculated straight from the database.
3. The whole database is supposed to be (locally) back uped every night. The date of the last backup run is gathered.
4. Finally a few data quality indicators (number of active patients, number of new patients, number of visits) as defined by MOH are calculated.


Java reporting

PDF reports can be generated by the comand line (without the OpenMRS module). For this the script generate-report.sh with start and end dates as well as the file name of the output PDF as paramaters can be invoked from the installation directory.

As the report has a static one-page layout without any sopisticated layout, the PDF generation Apache PDF Box is used to generate the report.

The main class and entry point for parsing and reporting is the class org.openmrs.module.emtfrontend.Emt. This decomposes and filters the log file and calculates the indicators for the given period.


OpenMRS module

The OpenMRS module is a simplistic module (contains only 2 pages in the OpenMRS admin section) allowing easier access to the report generation and the re-configuration of clinic hours. It follows the default OpenMRS conventions for module development and was derived from the basic module example. From here the logic for the Java reporting is simply invoked and the result as a PDF returned to the browser. The module is not required for the operation; it simply provides easier access to the report generation.



EMT log file

The dedicated log file for the EMT is stored under the file /home/hc-admin/emt.log (might be wise to move it to the /var/log directory if permissions are sorted out) and is basically a CSV file. Every row represent one monitoring event and always contains the current date and a unique system ID (system name followed by the MAC address if eth0; this ID can be helful later on if multiple log files are automatically combined and aggregated). Following htese two values a event type distinguishes between a STARTUP, a general HEARTBEAT, and the dedicated OpenMRS and MySQL heartbeat messages. Each of these types have their own values as the payloads.

$NOW;$SYSTEM_ID;STARTUP;$SHUTDOWN_STATUS
$NOW;$SYSTEM_ID;HEARTBEAT;$SYS_LOAD;$NUMBER_PROCESSORS;$MEM_TOTAL;$MEM_FREE;$SDA1_DISK_TOTAL;$SDA1_DISK_FREE;$SDA1_DISK_USE;$DISK_TOTAL;$DISK_FREE;$DISK_USE
$NOW;$SYSTEM_ID;OPENMRS-HEARTBEAT;$OPENMRS_STATUS;$NUMBER_ENCOUNTERS;$NUMBER_OBS;$NUMBER_USERS;$BACKUP_STATUS;$NUMBER_ACTIVE_PATIENTS;$NUMBER_NEW_PATIENTS;$NUMBER_VISITS


Releasing a new version

To release a new version both backend (shell) and frontend (OpenMRS module) needs to be packaged separately. (I guess there are ways to make the Maven spit out the shell backend, but didn't figure it out). Keep the order of these steps as one builds upon the results of the previous step:
1. Adjust version numbers
2. Run top level pom.xml to create an ordinary OpenMRS module for the EMT frontend (and to compile the class files for step #4). The omod can be found under ./omod/target/emtfrontend...
3. Run shrink-omod.sh to strip down the 10 MB Apache PDFbox of the omod to the bare required minimum (optional but highly required as it speeds up the deployment of the omod)
4. Run create-shell-backend-installation-package.sh to create the file ./install.sh. This install.sh is used to install/update the shell-backend and besides the installation logic contains (!) the java binaries wrapped in a TGZ part.

The shell scripts are only tested on a MacOS X system, but they should (with maybe minimal adjustments) also work on Linux systems. For the brave ones a Cygwin port to Windows is also not too far of a stretch.


Installation process

The installation process itself is straight forward. But the shell backend assumes it is installed on a 'default' MOH server in Rwanda (mainly for the location of the openmrs-runtime.properties and the existance of the user hc-admin). With minimal changes it is possible to install the EMT on ordinary Ubuntu systems, but this is not part of a default installation.

Once the environment fits, running the shell script install.sh installs the backend and the omod can be deployed through OpenMRS ways. Now the system needs to collect some data, before any useful report could be generated.


Additional stuff

Thoughts on command last -x
The log file wtmp and its command last can be used to see changes in run levels and reboot and shutdown events. This is available on every system and in theory could be used for even historic evaluation of system uptime without any additional software and configuration. However: Even though the existence of crashes are detectable (reboot entry without preceding shutdowns), the time when the crash happened is not available (unless a user was logged in through a terminal (PTS)). Therefore the system uptime can only be calculates when there was not crash/unexpected shutdown, which unfortunately leaves out the most interesting part.

Example output with stripped down content for reboots, shutdowns and crashes:

reboot 12:29
moh-admi pts/0        192.168.39.1     Tue Apr 29 12:30   still logged in   
runlevel (to lvl 2)   3.2.0-60-generic Tue Apr 29 12:29 - 12:30  (00:00)    
reboot   system boot  3.2.0-60-generic Tue Apr 29 12:29 - 12:30  (00:00)    
shutdown system down  3.2.0-60-generic Tue Apr 29 12:29 - 12:29  (00:00)    
runlevel (to lvl 6)   3.2.0-60-generic Tue Apr 29 12:29 - 12:29  (00:00)    

shutdown 12:31

startup 12:36
moh-admi pts/0        192.168.39.1     Tue Apr 29 12:36   still logged in   
runlevel (to lvl 2)   3.2.0-60-generic Tue Apr 29 12:34 - 12:36  (00:01)    
reboot   system boot  3.2.0-60-generic Tue Apr 29 12:34 - 12:36  (00:01)    
shutdown system down  3.2.0-60-generic Tue Apr 29 12:31 - 12:34  (00:03)    
runlevel (to lvl 0)   3.2.0-60-generic Tue Apr 29 12:31 - 12:31  (00:00)    
moh-admi pts/0        192.168.39.1     Tue Apr 29 12:30 - down   (00:00)    
runlevel (to lvl 2)   3.2.0-60-generic Tue Apr 29 12:29 - 12:31  (00:01)    
reboot   system boot  3.2.0-60-generic Tue Apr 29 12:29 - 12:31  (00:01)    
shutdown system down  3.2.0-60-generic Tue Apr 29 12:29 - 12:29  (00:00)    
runlevel (to lvl 6)   3.2.0-60-generic Tue Apr 29 12:29 - 12:29  (00:00)    

forced shutdown 12:40

startup 12:49
moh-admi pts/0        192.168.39.1     Tue Apr 29 12:49   still logged in   
runlevel (to lvl 2)   3.2.0-60-generic Tue Apr 29 12:48 - 12:49  (00:01)    
reboot   system boot  3.2.0-60-generic Tue Apr 29 12:48 - 12:49  (00:01)    
moh-admi pts/0        192.168.39.1     Tue Apr 29 12:36 - crash  (00:12)    
runlevel (to lvl 2)   3.2.0-60-generic Tue Apr 29 12:34 - 12:48  (00:13)    
reboot   system boot  3.2.0-60-generic Tue Apr 29 12:34 - 12:49  (00:15)    
shutdown system down  3.2.0-60-generic Tue Apr 29 12:31 - 12:34  (00:03)    
runlevel (to lvl 0)   3.2.0-60-generic Tue Apr 29 12:31 - 12:31  (00:00)    
moh-admi pts/0        192.168.39.1     Tue Apr 29 12:30 - down   (00:00)    
runlevel (to lvl 2)   3.2.0-60-generic Tue Apr 29 12:29 - 12:31  (00:01)    
reboot   system boot  3.2.0-60-generic Tue Apr 29 12:29 - 12:31  (00:01)    
shutdown system down  3.2.0-60-generic Tue Apr 29 12:29 - 12:29  (00:00)    
runlevel (to lvl 6)   3.2.0-60-generic Tue Apr 29 12:29 - 12:29  (00:00)    
